<!DOCTYPE html>
<html lang="en">
<head>
  <script src="script.js"></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Research</title>
  <link rel="stylesheet" href="styles1.css">
</head>
<body>

  <header>
    <nav>
      <ul>
        <li><a href="#overview">Overview</a></li>
        <li><a href="#publications">Publications</a></li>
        <li><a href="#insights">Insights</a></li>
        <li><a href="#inprogress">In Progress</a></li>
        <li><a href="#toread">To Read</a></li>
      </ul>
    </nav>
    <h1><center>AI Research</center></h1>
  </header>

  <main>

    <section id="overview">
      <h2>Overview</h2>
      <img src="1234.jpg">
      <p>Am I an AI research expert...no, have I ever trained a Large language model...I can't afford it...do i have 10+ years experience in Machine Learning...ehm no.</p>
      <p>But I have PASSION, a VISION, AGGRESION *with There is no passion meme voice* plus I believe in just randomly throwing myself into things that I don't know its the best way to learn...well anything, you probably did too once like when you were learning how to walk as a baby. Besides there is no time to learn anything in the entire history of humanity thanks to GPT (Our lord and saviour) so might as well take advantage and just combine Mr Douglas Hofstadter's analysis ( book title : Analogy as the Fuel and Fire of Thinking  (A must read btw)) to learn anything with GPT.</p>
      <img src="55.png">
    </section>

    <section id="publications">
      <h2>Research Papers</h2>
      <article id="paper1">
        <h3>ReFine: Re-randomization before Fine-tuning for Cross-domain Few-shot Learning</h3>
        <img src="paper1.jpg" alt="Research Paper 1">
        <p>This paper is like a chameleon, adapting to its environment. It discusses a method for cross-domain few-shot learning where the parameters fitted on the source domain are re-randomized before adapting to the target data, facilitating fine-tuning on the target domain and improving few-shot performance.</p>
        <a href="https://dx.doi.org/10.1145/3511808.3557681">View Full Paper</a>
      </article>
      <article id="paper2">
        <h3>Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees</h3>
        <p>This paper is like a seasoned explorer, charting unknown territories. It presents a novel method for high-dimensional planning problems, using a neural network to guide the exploration-exploitation trade-off. The authors demonstrate that their approach outperforms traditional methods in a variety of complex environments.</p>
        <a href="https://arxiv.org/pdf/2306.13575.pdf">View Full Paper</a>
      </article>

      <article id="paper3">
        <h3>Thought Cloning: Learning to Think while Acting by Imitating human Thinking</h3>
        <p>This paper is like a skilled mimic, learning from human thought processes. It introduces the concept of Thought Cloning, where AI agents learn to act and think from demonstrations where humans think out loud while acting. The authors argue that this approach can improve the generalization, exploration, planning, and adaptation capabilities of AI agents.</p>
        <a href="https://arxiv.org/pdf/2306.00323.pdf">View Full Paper</a>
      </article>

      <article id="paper4">
        <h3>Forward Thinking: Building Deep Random Forests</h3>
        <p>This paper is like a master builder, constructing sophisticated structures. It presents a framework called "forward thinking" for deep learning that extends the architectural flexibility and sophisticationof deep neural networks. It allows for different types of learning functions in the network, other than neurons, and the ability to adaptively deepen the network as needed to improve results.</p>
        <a href="https://arxiv.org/pdf/1705.07366.pdf">View Full Paper</a>
      </article>

      <article id="paper5">
        <h3>SpQR: Practical Near-Lossless Compression of Large Language Models</h3>
        <p>This paper is like a skilled magician, making large objects disappear. It presents SpQR, a near-lossless compression algorithm for Large Language Models (LLMs). The authors argue that SpQR allows for the deployment of high-quality LLMs in the 7-13B parameters range to memory-limited devices such as laptops and phones.</p>
        <a href="https://arxiv.org/pdf/2306.03078.pdf">View Full Paper</a>
      </article>

      <!-- Additional publications -->
    </section>

    <section id="insights">
      <h2>Research Insights</h2>
      <article id="insight1">
        <h3>Universal Language Model Fine-tuning for Text Classification</h3>
        <p>Imagine a Swiss Army knife, versatile and adaptable. That's what this paper proposes - a method called ULMFiT for applying transfer learning to any NLP task. It's like having a tool that can adapt to any situation, making it incredibly powerful.</p>
        <a href="https://arxiv.org/abs/1801.06146">Read More</a>
      </article>

      <!-- Additional research insights -->
    </section>

    <section id="inprogress">
      <h2>Research In Progress</h2>
      <article id="inprogress1">
        <h3>Transformers discover an elementary calculation system exploiting local attention and grid-like problem representation</h3>
        <img src="inprogress1.jpg" alt="Research In Progress 1">
        <p>Imagine a child learning to add numbers for the first time. This paper explores whether transformers can learn to solve problems recursively, much like a child learning to perform multi-digit addition. It's a fascinating exploration into the learning capabilities of AI.</p>
        <a href="https://dx.doi.org/10.1109/IJCNN55064.2022.9892619">Read More</a>
      </article>

     <!-- Additional ongoing research projects -->
    </section>

    <section id="toread">
      <h2>Need to Read</h2>
      <article id="toread1">
        <h3>Thought cloning: Learning to Think while Acting by Imitating human Thinking</h3>
        <p>This paper title sounds like a sci-fi movie plot, doesn't it? I couldn't find a specific paper with this title, but the concept of thought cloning generally refers to the idea of creating AI models that can mimic human thought processes. It's like creating a digital twin of the human mind. I need to read more about this!</p>
      </article>

      <!-- Additional papers to read -->
    </section>

  </main>

  <footer>
    <p>Â© 2023 The Alchemists Odyssey</p>
  </footer>

</body>
</html>
